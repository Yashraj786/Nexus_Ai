<%# app/views/help/case_study.html.erb %>
<div class="container mx-auto px-4 py-8">
  <div class="prose prose-invert max-w-none">
    <h1>Nexus AI: A Case Study in Stability & Observability</h1>
    
    <h2>Overview</h2>
    <p>Nexus AI is a conversational AI platform that leverages multiple external AI providers. This case study documents the process of transforming the initial prototype into a production-ready application with a strong focus on stability, observability, and a structured feedback loop. The goal was to build a reliable system that could gracefully handle the inherent unreliability of external services and provide clear insights into its operational health.</p>

    <h2>Goals</h2>
    <ul>
      <li><strong>Stability:</strong> Ensure the application remains functional and responsive, even when external AI APIs are slow or unavailable.</li>
      <li><strong>Observability:</strong> Implement tools to monitor the health of the AI integration, track key performance indicators (KPIs), and quickly diagnose issues.</li>
      <li><strong>Feedback-Driven Development:</strong> Establish a process for collecting, triaging, and acting on user feedback to continuously improve the platform.</li>
    </ul>

    <h2>Architecture & Stability Features</h2>
    <ul>
      <li><strong>Robust Retry Logic:</strong> Implemented a manual retry mechanism in `AiResponseJob` with exponential backoff and jitter. This prevents hammering external APIs during outages and ensures a graceful degradation of service.</li>
      <li><strong>Input Sanitization:</strong> The `Message` model now includes a `sanitize_content` method that strips non-printable characters and control characters, in addition to HTML tags, to prevent XSS and other injection attacks.</li>
      <li><strong>Centralized Error Handling:</strong> All AI API calls are wrapped in a service object (`Ai::GenerateResponseService`) that returns a structured response, separating success and failure cases. This prevents uncaught exceptions from crashing the application.</li>
    </ul>

    <h2>Observability & SLOs</h2>
    <p>We defined two primary Service Level Objectives (SLOs) to measure the reliability of our AI integration:</p>
    <ul>
      <li><strong>p95 Latency:</strong> 95% of AI responses should be delivered in under 3 seconds.</li>
      <li><strong>Success Rate:</strong> 99.5% of all AI API requests should be successful.</li>
    </ul>
    <p>To monitor these SLOs, we implemented:</p>
    <ul>
      <li><strong>Structured Logging:</strong> A custom `ApiEventLogger` logs all significant events (request start/end, latency, success/failure, retries) to a dedicated `log/api_events.log` file in JSON format.</li>
      <li><strong>Admin Dashboard:</strong> A real-time dashboard at `/admin/dashboard` parses the event log to display SLO compliance, requests/min, failures/min, average latency, and top error types.</li>
    </ul>

    <h2>Beta Process</h2>
    <p>We ran a small, structured beta with a select group of users to validate our stability and observability features. The process was as follows:</p>
    <ol>
      <li><strong>Onboarding:</strong> Beta users were provided with a `/help/beta` page that outlined the scope of the beta, our SLOs, and how to report issues.</li>
      <li><strong>Feedback Collection:</strong> A "Report a Problem" button in the chat UI allowed users to submit feedback directly from the application. Each feedback entry was tied to a specific chat session and included a snapshot of recent API events for easier debugging.</li>
      <li><strong>Triage & Review:</strong> Feedback was categorized and prioritized in an admin "Feedback Inbox." This allowed us to identify the most pressing issues and correlate user-reported problems with our SLO data.</li>
    </ol>

    <h2>Outcomes & Next Steps</h2>
    <p>The beta was a success. We were able to identify and fix several bugs and performance bottlenecks before a wider release. The observability dashboard and feedback loop were instrumental in this process.</p>
    <p>The next steps for Nexus AI include:</p>
    <ul>
      <li><strong>Automated Alerting:</strong> Integrate automated alerts (e.g., via email or Slack) when SLOs are breached.</li>
      <li><strong>Rate Limiting:</strong> Implement per-user rate limiting to prevent abuse.</li>
      <li><strong>Expanded Test Suite:</strong> Add more end-to-end regression tests to cover a wider range of user scenarios.</li>
    </ul>
  </div>
</div>
